{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChingWingYeung/Assignment2WebCrawler/blob/main/etl_dimension.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cfa00868",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfa00868",
        "outputId": "bca1d76e-ef9c-4b76-b4ca-e46cccb2a884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ],
      "source": [
        "# If using Google CoLab, run the following lines to set up Authentication with GCP\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If using Google CoLab, import these  modules for BigQuery\n",
        "%load_ext google.cloud.bigquery\n",
        "%load_ext google.colab.data_table"
      ],
      "metadata": {
        "id": "8fEahS8G3Cjv"
      },
      "id": "8fEahS8G3Cjv",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If using the native Google BigQuery API module:\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "import pandas as pd\n",
        "import os\n",
        "import pyarrow\n",
        "import logging\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "1dhS_W_y2-Zw"
      },
      "id": "1dhS_W_y2-Zw",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If using a service account key file, save the path to that file in credentials.py and import credentials\n",
        "path_to_service_account_key_file = \"D:\\Desktop\\genuine-period-442417-u9-ccc724799f16.json\"\n",
        "import credentials\n"
      ],
      "metadata": {
        "id": "r405QK10hDNU"
      },
      "id": "r405QK10hDNU",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "46960187",
      "metadata": {
        "id": "46960187"
      },
      "outputs": [],
      "source": [
        "# Set the name of the dimension\n",
        "dimension_name = 'agency'\n",
        "\n",
        "# Set the name of the surrogate key\n",
        "surrogate_key = f\"{dimension_name}_dim_id\"\n",
        "\n",
        "# Set the name of the business key\n",
        "business_key = f'{dimension_name}_id'\n",
        "\n",
        "# Set the GCP Project, dataset and table name\n",
        "gcp_project = 'My First Project'\n",
        "bq_dataset = '311_warehouse'\n",
        "table_name = f\"{dimension_name}_dimension\"\n",
        "# Construct the full BigQuery path to the table\n",
        "dimension_table_path = f\"{gcp_project}.{bq_dataset}.{table_name}\"\n",
        "\n",
        "# Set the path to the source data files. Use double-slash for Windows paths C:\\\\myfolder\n",
        "# For Linux use forward slashes    /home/username/python_etl\n",
        "# For Mac use forward slashes      /users/username/python_etl\n",
        "# file_source_path = 'c:\\\\Python_ETL'\n",
        "# file_source_path = 'C:\\\\Users\\\\rholo\\\\OneDrive\\\\Documents\\\\classes\\\\4400\\\\311'\n",
        "file_source_path = 'D:\\\\Desktop'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "060f8796",
      "metadata": {
        "id": "060f8796"
      },
      "outputs": [],
      "source": [
        "# Set up logging\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "current_date = datetime.today().strftime('%Y%m%d')\n",
        "log_filename = \"_\".join([\"etl\",dimension_name,current_date])+\".log\"\n",
        "logging.basicConfig(filename=log_filename, encoding='utf-8', format='%(asctime)s %(message)s', level=logging.DEBUG)\n",
        "logging.info(\"=========================================================================\")\n",
        "logging.info(f\"Starting ETL Run for dimension {dimension_name} on date {current_date}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bc1e72b6",
      "metadata": {
        "id": "bc1e72b6"
      },
      "outputs": [],
      "source": [
        "def load_csv_data_file(logging: logging.Logger,\n",
        "                      file_source_path: str,\n",
        "                      file_name: str,\n",
        "                      df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    load_csv_data_file\n",
        "    Accepts a file source path and a file name\n",
        "    Loads the file into a data frame\n",
        "    Exits the program on error\n",
        "    Returns the dataframe\n",
        "    \"\"\"\n",
        "    file_source = os.path.join(file_source_path, file_name)\n",
        "    logging.info(f\"Reading source data file: {file_source}\")\n",
        "    # Read in the source data file for the customers data\n",
        "    try:\n",
        "        df = pd.read_csv(file_source)\n",
        "        # Set all of the column names to lower case letters\n",
        "        df = df.rename(columns=str.lower)\n",
        "        logging.info(f\"Read {len(df)} records from source data file: {file_source}\")\n",
        "        return df\n",
        "    except:\n",
        "        logging.error(f\"Failed to read file: {file_source}\")\n",
        "        # os._exit(-1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "436af56f",
      "metadata": {
        "id": "436af56f"
      },
      "outputs": [],
      "source": [
        "def transform_data(logging: logging.Logger,\n",
        "                   df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    transform_data\n",
        "    Accepts a data frame\n",
        "    Performs any specific cleaning and transformation steps on the dataframe\n",
        "    Returns the modified dataframe\n",
        "    \"\"\"\n",
        "    # Convert the date_of_birth to a datetime64 data type. 2012-08-21 04:12:16.827\n",
        "    logging.info(\"Transforming dataframe.\")\n",
        "    # Select the columns for this dimension\n",
        "    column_list = ['agency','agency_name']\n",
        "    df = df[column_list]\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0d1bee48",
      "metadata": {
        "id": "0d1bee48"
      },
      "outputs": [],
      "source": [
        "def create_bigquery_client(logging):\n",
        "    \"\"\"\n",
        "    create_bigquery_client\n",
        "    Creates a BigQuery client using the path to the service account key file\n",
        "    for credentials.\n",
        "    Returns the BigQuery client object\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # If authenticating using a service account key file, use the following code:\n",
        "        # bqclient = bigquery.Client.from_service_account_json(credentials.path_to_service_account_key_file)\n",
        "        # Google Colab authentication already completed\n",
        "        bqclient = bigquery.Client(gcp_project)\n",
        "        logging.info(\"Created BigQuery Client: %s\",bqclient)\n",
        "        return bqclient\n",
        "    except Exception as err:\n",
        "        logging.error(\"Failed to create BigQuery Client.\", err)\n",
        "        # os._exit(-1)\n",
        "    return bqclient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3578f519",
      "metadata": {
        "id": "3578f519"
      },
      "outputs": [],
      "source": [
        "def upload_bigquery_table(logging, bqclient, table_path, write_disposition, df):\n",
        "    \"\"\"\n",
        "    upload_bigquery_table\n",
        "    Accepts a path to a BigQuery table, the write disposition and a dataframe\n",
        "    Loads the data into the BigQuery table from the dataframe.\n",
        "    for credentials.\n",
        "    The write disposition is either\n",
        "    write_disposition=\"WRITE_TRUNCATE\"  Erase the target data and load all new data.\n",
        "    write_disposition=\"WRITE_APPEND\"    Append to the existing table\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"Creating BigQuery Job configuration with write_disposition=%s\", write_disposition)\n",
        "        # Set up a BigQuery job configuration with the write_disposition.\n",
        "        job_config = bigquery.LoadJobConfig(write_disposition=write_disposition)\n",
        "        # Submit the job\n",
        "        logging.info(\"Submitting the BigQuery job\")\n",
        "        job = bqclient.load_table_from_dataframe(df, table_path, job_config=job_config)\n",
        "        # Show the job results\n",
        "        logging.info(\"Job  results: %s\",job.result())\n",
        "    except Exception as err:\n",
        "        logging.error(\"Failed to load BigQuery Table. %s\", err)\n",
        "        #os._exit(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3d37a312",
      "metadata": {
        "id": "3d37a312"
      },
      "outputs": [],
      "source": [
        "def bigquery_table_exists(bqclient, table_path):\n",
        "    \"\"\"\n",
        "    bigquery_table_exists\n",
        "    Accepts a path to a BigQuery table\n",
        "    Checks if the BigQuery table exists.\n",
        "    Returns True or False\n",
        "    \"\"\"\n",
        "    try:\n",
        "        bqclient.get_table(table_path)  # Make an API request.\n",
        "        return True\n",
        "    except NotFound:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "79339f5c",
      "metadata": {
        "id": "79339f5c"
      },
      "outputs": [],
      "source": [
        "def query_bigquery_table(logging, table_path, bqclient, surrogate_key):\n",
        "    \"\"\"\n",
        "    query_bigquery_table\n",
        "    Accepts a path to a BigQuery table and the name of the surrogate key\n",
        "    Queries the BigQuery table but leaves out the update_timestamp and surrogate key columns\n",
        "    Returns the dataframe\n",
        "    \"\"\"\n",
        "    bq_df = pd.DataFrame\n",
        "    sql_query = 'SELECT * EXCEPT ( update_timestamp, '+surrogate_key+') FROM `' + table_path + '`'\n",
        "    logging.info(\"Running query: %s\", sql_query)\n",
        "    try:\n",
        "        bq_df = bqclient.query(sql_query).to_dataframe()\n",
        "    except Exception as err:\n",
        "        logging.info(\"Error querying the table. %s\", err)\n",
        "    return bq_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "484a7699",
      "metadata": {
        "id": "484a7699"
      },
      "outputs": [],
      "source": [
        "def add_surrogate_key(df, dimension_name='customers', offset=1):\n",
        "    \"\"\"\n",
        "    add_surrogate_key\n",
        "    Accepts a data frame and inserts an integer identifier as the first column\n",
        "    Returns the modified dataframe\n",
        "    \"\"\"\n",
        "    # Reset the index to count from 0\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    # Add the new surrogate key starting from offset\n",
        "    df.insert(0, dimension_name+'_dim_id', df.index+offset)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "de2d5c71",
      "metadata": {
        "id": "de2d5c71"
      },
      "outputs": [],
      "source": [
        "def add_update_date(df, current_date):\n",
        "    \"\"\"\n",
        "    add_update_date\n",
        "    Accepts a data frame and inserts the current date as a new field\n",
        "    Returns the modified dataframe\n",
        "    \"\"\"\n",
        "    df['update_date'] = pd.to_datetime(current_date)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b2125588",
      "metadata": {
        "id": "b2125588"
      },
      "outputs": [],
      "source": [
        "def add_update_timestamp(df):\n",
        "    \"\"\"\n",
        "    add_update_timestamp\n",
        "    Accepts a data frame and inserts the current datetime as a new field\n",
        "    Returns the modified dataframe\n",
        "    \"\"\"\n",
        "    df['update_timestamp'] = pd.to_datetime('now', utc=True).replace(microsecond=0)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a93def27",
      "metadata": {
        "id": "a93def27"
      },
      "outputs": [],
      "source": [
        "def build_new_table(logging, bqclient, dimension_table_path, dimension_name, df):\n",
        "    \"\"\"\n",
        "    build_new_table\n",
        "    Accepts a path to a dimensional table, the dimension name and a data frame\n",
        "    Add the surrogate key and a record timestamp to the data frame\n",
        "    Inserts the contents of the dataframe to the dimensional table.\n",
        "    \"\"\"\n",
        "    logging.info(\"Target dimension table %s does not exit\", dimension_table_path)\n",
        "    # Add a surrogate key\n",
        "    df = add_surrogate_key(df, dimension_name, 1)\n",
        "    # Add the update timestamp\n",
        "    df = add_update_timestamp(df)\n",
        "    # Upload the dataframe to the BigQuery table\n",
        "    upload_bigquery_table(logging, bqclient, dimension_table_path, \"WRITE_TRUNCATE\", df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "dcffe5fe",
      "metadata": {
        "id": "dcffe5fe"
      },
      "outputs": [],
      "source": [
        "def insert_existing_table(logging, bqclient, dimension_table_path, dimension_name, surrogate_key, df):\n",
        "    \"\"\"\n",
        "    insert_existing_table\n",
        "    Accepts a path to a dimensional table, the dimension name and a data frame\n",
        "    Compares the new data to the existing data in the table.\n",
        "    Inserts the new/modified records to the existing table\n",
        "    \"\"\"\n",
        "    bq_df = pd.DataFrame\n",
        "    logging.info(\"Target dimension table %s exits. Checking for differences.\", dimension_table_path)\n",
        "    # Fetch the existing table\n",
        "    bq_df = query_bigquery_table(logging, dimension_table_path, bqclient, surrogate_key)\n",
        "    # Compare with the new data set\n",
        "    new_records_df = pd.concat([df,bq_df]).drop_duplicates(keep=False)\n",
        "    logging.info(\"Found %d new records.\", new_records_df.shape[0])\n",
        "    if new_records_df.shape[0] > 0:\n",
        "        # Set the surrogate key for the new records. bq_df.shape[0] is number of records already in the database\n",
        "        new_surrogate_key_value = bq_df.shape[0]+1\n",
        "        new_records_df = add_surrogate_key(new_records_df, dimension_name, new_surrogate_key_value)\n",
        "        # Add the current date for the new records\n",
        "        new_records_df = add_update_timestamp(new_records_df)\n",
        "        # Upload the new records into the dimension table\n",
        "        upload_bigquery_table(logging, bqclient, dimension_table_path, \"WRITE_APPEND\", new_records_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "55edebde",
      "metadata": {
        "scrolled": true,
        "id": "55edebde",
        "outputId": "61a62987-48e5-414b-8467-9c8d04e89417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'type' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-f351a03b5488>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_data_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_source_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"311_rodent_complaint.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Transform the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Create the BigQuery Client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mbqclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_bigquery_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1a102d7d826e>\u001b[0m in \u001b[0;36mtransform_data\u001b[0;34m(logging, df)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Select the columns for this dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcolumn_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'agency'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'agency_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Remove duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# Program main\n",
        "# Load the CSV File into a dataframe\n",
        "# Transform the Dataframe\n",
        "# Create a BigQuery client\n",
        "# See if the target dimension table exists\n",
        "#    If not exists, load the data into a new table\n",
        "#    If exists, insert new records into the table\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.DataFrame\n",
        "    # Load in the data file\n",
        "    df = load_csv_data_file(logging, file_source_path, \"311_rodent_complaint.csv\", df)\n",
        "    # Transform the data\n",
        "    df = transform_data(logging, df)\n",
        "    # Create the BigQuery Client\n",
        "    bqclient = create_bigquery_client(logging)\n",
        "    # See if the target dimensional table exists\n",
        "    target_table_exists = bigquery_table_exists(bqclient, dimension_table_path  )\n",
        "    # If the target dimension table does not exist, load all of the data into a new table\n",
        "    if not target_table_exists:\n",
        "        build_new_table(logging, bqclient, dimension_table_path, dimension_name, df)\n",
        "    # If the target table exists, then perform an incremental load\n",
        "    if target_table_exists:\n",
        "        insert_existing_table(logging, bqclient, dimension_table_path, dimension_name, surrogate_key, df)\n",
        "    # Flush the log file\n",
        "    logging.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"mnist_test.csv\")\n",
        "display(df)"
      ],
      "metadata": {
        "id": "0rBHpzAN-8kv",
        "outputId": "c83ae853-5551-4b4f-b8f0-8065eaa1a96a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "id": "0rBHpzAN-8kv",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'mnist_test.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-5f337fcfea9b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mnist_test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist_test.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc79cae8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc79cae8",
        "outputId": "7ce81445-c9d0-41c6-a22b-8f3a56d0be92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-29 15:56:28,977 Started reading table 'handy-bonbon-142723._5ef0660eeeb79c6dc70d5e60af1e07f5112fb556.anon7a21409bf6c5b560a28d311262b7ed445e8d2ce7cba18071e5306baf5ac457ce' with BQ Storage API session 'projects/handy-bonbon-142723/locations/us/sessions/CAISDEFYaEtjNGhOSEQzNRoCaXcaAmpk'.\n",
            "2023-03-29 15:56:29,719 Found 0 new records.\n",
            "2023-03-29 15:58:01,091 Making request: GET http://169.254.169.254\n",
            "2023-03-29 15:58:01,099 Making request: GET http://metadata.google.internal/computeMetadata/v1/project/project-id\n",
            "2023-03-29 15:58:01,102 Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true\n",
            "2023-03-29 15:58:01,104 Starting new HTTP connection (1): metadata.google.internal:80\n",
            "2023-03-29 15:58:01,265 http://metadata.google.internal:80 \"GET /computeMetadata/v1/instance/service-accounts/default/?recursive=true HTTP/1.1\" 200 198\n",
            "2023-03-29 15:58:01,266 Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/professorholowczak@gmail.com/token?scopes=email\n",
            "2023-03-29 15:58:01,416 http://metadata.google.internal:80 \"GET /computeMetadata/v1/instance/service-accounts/professorholowczak@gmail.com/token?scopes=email HTTP/1.1\" 200 418\n",
            "2023-03-29 15:58:01,470 =========================================================================\n",
            "2023-03-29 15:58:01,470 Starting ETL Run for dimension agency on date 20230329\n",
            "2023-03-29 15:58:01,598 Reading source data file: /content/311_bicycle_complaints_2017.csv\n",
            "2023-03-29 15:58:01,633 Read 1952 records from source data file: /content/311_bicycle_complaints_2017.csv\n",
            "2023-03-29 15:58:01,633 Transforming dataframe.\n",
            "2023-03-29 15:58:01,636 Making request: GET http://169.254.169.254\n",
            "2023-03-29 15:58:01,637 Making request: GET http://metadata.google.internal/computeMetadata/v1/project/project-id\n",
            "2023-03-29 15:58:01,639 Created BigQuery Client: <google.cloud.bigquery.client.Client object at 0x7f0bd179a160>\n",
            "2023-03-29 15:58:01,640 Converted retries value: 3 -> Retry(total=3, connect=None, read=None, redirect=None, status=None)\n",
            "2023-03-29 15:58:01,640 Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true\n",
            "2023-03-29 15:58:01,642 Starting new HTTP connection (1): metadata.google.internal:80\n",
            "2023-03-29 15:58:02,483 http://metadata.google.internal:80 \"GET /computeMetadata/v1/instance/service-accounts/default/?recursive=true HTTP/1.1\" 200 198\n",
            "2023-03-29 15:58:02,484 Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/professorholowczak@gmail.com/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fbigquery%2Chttps%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform\n",
            "2023-03-29 15:58:02,648 http://metadata.google.internal:80 \"GET /computeMetadata/v1/instance/service-accounts/professorholowczak@gmail.com/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fbigquery%2Chttps%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform HTTP/1.1\" 200 418\n",
            "2023-03-29 15:58:02,651 Starting new HTTPS connection (1): bigquery.googleapis.com:443\n",
            "2023-03-29 15:58:02,855 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/datasets/nyc_311_complaints_dw/tables/agency_dimension?prettyPrint=false HTTP/1.1\" 404 None\n",
            "2023-03-29 15:58:02,856 Target dimension table handy-bonbon-142723.nyc_311_complaints_dw.agency_dimension does not exit\n",
            "2023-03-29 15:58:02,863 Creating BigQuery Job configuration with write_disposition=WRITE_TRUNCATE\n",
            "2023-03-29 15:58:02,863 Submitting the BigQuery job\n",
            "2023-03-29 15:58:04,386 https://bigquery.googleapis.com:443 \"POST /upload/bigquery/v2/projects/handy-bonbon-142723/jobs?uploadType=multipart HTTP/1.1\" 200 1591\n",
            "2023-03-29 15:58:04,581 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/jobs/5b483b56-50cb-412d-a2c8-1391c7603cae?location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-03-29 15:58:04,582 Retrying due to , sleeping 0.5s ...\n",
            "2023-03-29 15:58:05,262 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/jobs/5b483b56-50cb-412d-a2c8-1391c7603cae?location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-03-29 15:58:05,263 Retrying due to , sleeping 1.0s ...\n",
            "2023-03-29 15:58:06,497 https://bigquery.googleapis.com:443 \"GET /bigquery/v2/projects/handy-bonbon-142723/jobs/5b483b56-50cb-412d-a2c8-1391c7603cae?location=US&prettyPrint=false HTTP/1.1\" 200 None\n",
            "2023-03-29 15:58:06,498 Job  results: LoadJob<project=handy-bonbon-142723, location=US, id=5b483b56-50cb-412d-a2c8-1391c7603cae>\n"
          ]
        }
      ],
      "source": [
        "# Check the log. Use cat, head or tail\n",
        "!tail -35 etl_agency_20230329.log\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}